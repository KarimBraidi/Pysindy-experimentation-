{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Trying to approximate Lorentz using SINDy**\n",
    "\n",
    "Note before reading the code: This code is experimental, I tried using small number of data with different approaches to approximate the Lorentz equations. If I used high number of samples the code would've converged on cell 2 and no further experimentation is needed."
   ],
   "id": "8259e9b7aea5afa3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cell 1, construction of the feature space\n",
   "id": "bba905810252eec8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:18:02.655740Z",
     "start_time": "2025-10-28T20:18:02.105894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Cell 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "# File path\n",
    "file_path = r\"C:\\Users\\braid\\OneDrive\\Desktop\\lorenze_attractor.csv\\lorenze_attractor.csv\"\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop(df.columns[0], axis=1)  # drop sample index\n",
    "\n",
    "expected_cols = [\"x\", \"y\", \"z\", \"u\", \"t\"]\n",
    "for c in expected_cols:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing column '{c}' in CSV. Found: {df.columns.tolist()}\")\n",
    "\n",
    "x, y, z, u, t = [df[c].to_numpy() for c in expected_cols]\n",
    "dt = np.mean(np.diff(t))\n",
    "\n",
    "# Using Finite Difference for derivatives (FDM)\n",
    "def finite_diff(signal, dt):\n",
    "    deriv = np.zeros_like(signal)\n",
    "    deriv[1:-1] = (signal[2:] - signal[:-2]) / (2 * dt)\n",
    "    deriv[0] = (signal[1] - signal[0]) / dt\n",
    "    deriv[-1] = (signal[-1] - signal[-2]) / dt\n",
    "    return deriv\n",
    "\n",
    "dx, dy, dz, du = [finite_diff(var, dt) for var in [x, y, z, u]]\n",
    "\n",
    "# Building the feature space\n",
    "features = {\n",
    "    \"x\": x, \"y\": y, \"z\": z, \"u\": u, \"t\": t,\n",
    "    \"dx\": dx, \"dy\": dy, \"dz\": dz, \"du\": du\n",
    "}\n",
    "\n",
    "# Polynomial powers\n",
    "for var in [\"x\", \"y\", \"z\", \"u\"]:\n",
    "    features[f\"{var}^2\"] = df[var] ** 2\n",
    "    features[f\"{var}^3\"] = df[var] ** 3\n",
    "\n",
    "# Pairwise products\n",
    "vars_basic = [\"x\", \"y\", \"z\", \"u\"]\n",
    "for i, j in itertools.combinations_with_replacement(vars_basic, 2):\n",
    "    features[f\"{i}{j}\"] = df[i] * df[j]\n",
    "\n",
    "# Derivative interactions\n",
    "for var in vars_basic:\n",
    "    for dvar in [\"dx\", \"dy\", \"dz\", \"du\"]:\n",
    "        features[f\"{var}*{dvar}\"] = df[var] * features[dvar]\n",
    "\n",
    "# Cross derivative terms\n",
    "for i, j in itertools.combinations_with_replacement([\"dx\", \"dy\", \"dz\", \"du\"], 2):\n",
    "    features[f\"{i}*{j}\"] = features[i] * features[j]\n",
    "\n",
    "# Complex higher-order terms\n",
    "complex_terms = [\n",
    "    (\"x\", \"dx\", \"dy\"), (\"x\", \"dy\", \"dz\"), (\"y\", \"dx\", \"dz\"),\n",
    "    (\"z\", \"dx\", \"dy\"), (\"x\", \"y\", \"dz\"), (\"x\", \"z\", \"dy\"), (\"y\", \"z\", \"dx\")\n",
    "]\n",
    "for combo in complex_terms:\n",
    "    name = \"*\".join(combo)\n",
    "    term = np.ones_like(x)\n",
    "    for c in combo:\n",
    "        term *= features[c]\n",
    "    features[name] = term\n",
    "\n",
    "# Trigonometric terms\n",
    "for var in [\"x\", \"y\", \"z\"]:\n",
    "    features[f\"sin({var})\"] = np.sin(df[var])\n",
    "    features[f\"cos({var})\"] = np.cos(df[var])\n",
    "\n",
    "# Combine into DataFrame\n",
    "feature_df = pd.DataFrame(features)\n",
    "\n",
    "# DOWNSAMPLE ROWS TO 2000 SAMPLES\n",
    "feature_df_small = feature_df.iloc[:5000, :].reset_index(drop=True) # the original data from kaggle has 200,000 samples which is too much to compute later on\n",
    "\n",
    "# SAVE SMALL MATRIX\n",
    "save_dir = os.path.dirname(file_path)\n",
    "feature_path_small = os.path.join(save_dir, \"rich_feature_matrix_small.csv\")\n",
    "feature_df_small.to_csv(feature_path_small, index=False)\n",
    "\n",
    "print(f\" Downsampled feature matrix (5000 samples) saved at:\\n{feature_path_small}\")\n",
    "print(f\"Shape: {feature_df_small.shape}\")\n"
   ],
   "id": "85686067bc9cdd7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downsampled feature matrix (5000 samples) saved at:\n",
      "C:\\Users\\braid\\OneDrive\\Desktop\\lorenze_attractor.csv\\rich_feature_matrix_small.csv\n",
      "Shape: (5000, 66)\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cell 2, Applying the SINDY Algorithm, then comparing the feature library (built in) with the feature library I created",
   "id": "876524971c973d71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:18:02.991327Z",
     "start_time": "2025-10-28T20:18:02.662966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Cell 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "from pysindy import SINDy\n",
    "from pysindy.feature_library import PolynomialLibrary, IdentityLibrary\n",
    "from pysindy.optimizers import STLSQ\n",
    "\n",
    "\n",
    "# File path\n",
    "file_path = r\"C:\\Users\\braid\\OneDrive\\Desktop\\lorenze_attractor.csv\\lorenze_attractor.csv\"\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop(df.columns[0], axis=1)  # drop sample index\n",
    "\n",
    "expected_cols = [\"x\", \"y\", \"z\", \"u\", \"t\"]\n",
    "for c in expected_cols:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing column '{c}' in CSV. Found: {df.columns.tolist()}\")\n",
    "\n",
    "x, y, z, u, t = [df[c].to_numpy() for c in expected_cols]\n",
    "dt = np.mean(np.diff(t))\n",
    "\n",
    "# Using Finite Difference for derivatives (FDM)\n",
    "def finite_diff(signal, dt):\n",
    "    deriv = np.zeros_like(signal)\n",
    "    deriv[1:-1] = (signal[2:] - signal[:-2]) / (2 * dt)\n",
    "    deriv[0] = (signal[1] - signal[0]) / dt\n",
    "    deriv[-1] = (signal[-1] - signal[-2]) / dt\n",
    "    return deriv\n",
    "\n",
    "dx, dy, dz, du = [finite_diff(var, dt) for var in [x, y, z, u]]\n",
    "\n",
    "# 2. CUSTOM RICH FEATURE MATRIX GENERATION (THETA)\n",
    "\n",
    "# Building the feature space dictionary\n",
    "features = {\n",
    "    \"1\": np.ones_like(x), # Add constant term for bias\n",
    "    \"x\": x, \"y\": y, \"z\": z, \"u\": u, \"t\": t,\n",
    "    \"dx\": dx, \"dy\": dy, \"dz\": dz, \"du\": du\n",
    "}\n",
    "\n",
    "vars_basic = [\"x\", \"y\", \"z\", \"u\"]\n",
    "\n",
    "# Polynomial terms (up to degree 3)\n",
    "for var in vars_basic:\n",
    "    features[f\"{var}^2\"] = df[var] ** 2\n",
    "    features[f\"{var}^3\"] = df[var] ** 3\n",
    "\n",
    "# Pairwise products (degree 2)\n",
    "for i, j in itertools.combinations_with_replacement(vars_basic, 2):\n",
    "    if i != j: # Only cross-terms; x*x is already x^2\n",
    "        features[f\"{i}{j}\"] = df[i] * df[j]\n",
    "\n",
    "# Derivative interactions and higher order terms\n",
    "deriv_vars = [\"dx\", \"dy\", \"dz\", \"du\"]\n",
    "# Derivative interactions (e.g., x*dx)\n",
    "for var in vars_basic:\n",
    "    for dvar in deriv_vars:\n",
    "        features[f\"{var}*{dvar}\"] = df[var] * features[dvar]\n",
    "\n",
    "# Cross derivative terms (e.g., dx*dy)\n",
    "for i, j in itertools.combinations_with_replacement(deriv_vars, 2):\n",
    "    features[f\"{i}*{j}\"] = features[i] * features[j]\n",
    "\n",
    "# Complex higher-order terms (e.g., x*dx*dy)\n",
    "complex_terms = [\n",
    "    (\"x\", \"dx\", \"dy\"), (\"x\", \"dy\", \"dz\"), (\"y\", \"dx\", \"dz\"),\n",
    "    (\"z\", \"dx\", \"dy\"), (\"x\", \"y\", \"dz\"), (\"x\", \"z\", \"dy\"), (\"y\", \"z\", \"dx\")\n",
    "]\n",
    "for combo in complex_terms:\n",
    "    name = \"*\".join(combo)\n",
    "    term = np.ones_like(x)\n",
    "    for c in combo:\n",
    "        term *= features[c]\n",
    "    features[name] = term\n",
    "\n",
    "# Trigonometric terms\n",
    "for var in [\"x\", \"y\", \"z\"]:\n",
    "    features[f\"sin({var})\"] = np.sin(df[var])\n",
    "    features[f\"cos({var})\"] = np.cos(df[var])\n",
    "\n",
    "\n",
    "# Combine into DataFrame and DOWNSAMPLE\n",
    "feature_df = pd.DataFrame(features)\n",
    "N_SAMPLES = 5000\n",
    "feature_df_small = feature_df.iloc[:N_SAMPLES, :].reset_index(drop=True)\n",
    "\n",
    "# Separate Data for SINDy\n",
    "deriv_cols = [\"dx\", \"dy\", \"dz\", \"du\"]\n",
    "\n",
    "# State Variables (X) - Input for PolynomialLibrary\n",
    "X = feature_df_small[vars_basic].to_numpy()\n",
    "# Derivatives (X_dot) - Target for both models\n",
    "X_dot = feature_df_small[deriv_cols].to_numpy()\n",
    "\n",
    "# Custom Feature Matrix (Theta) - Input for IdentityLibrary\n",
    "# Note: Dropping the derivatives and time column\n",
    "custom_feature_names = [name for name in feature_df_small.columns if name not in deriv_cols and name != \"t\"]\n",
    "Theta = feature_df_small[custom_feature_names].to_numpy()\n",
    "\n",
    "\n",
    "# 3. SINDy MODEL COMPARISON\n",
    "\n",
    "# A. SINDy with Standard Feature Space (Polynomial Library)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"A. SINDy with Standard Polynomial Library (Degree 3)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "poly_library = PolynomialLibrary(degree=3, include_bias=True)\n",
    "\n",
    "# Initialize the SINDy model\n",
    "model_poly = SINDy(\n",
    "    optimizer=STLSQ(threshold=0.05),\n",
    "    feature_library=poly_library,\n",
    ")\n",
    "\n",
    "# Fit the model: Correctly passes feature_names and the time step 't=dt'\n",
    "model_poly.fit(X, x_dot=X_dot, feature_names=vars_basic, t=dt)\n",
    "\n",
    "print(\"\\n*** Discovered Equations (Polynomial Library) ***\")\n",
    "model_poly.print(lhs=deriv_cols)\n",
    "\n",
    "\n",
    "# B. SINDy with Custom Rich Feature Matrix ( My feature library)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"B. SINDy with Custom Rich Feature Matrix (Your Theta)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize SINDy model (Uses IdentityLibrary for pre-computed Theta)\n",
    "model_custom = SINDy(\n",
    "    optimizer=STLSQ(threshold=0.005),\n",
    "    feature_library=IdentityLibrary(),\n",
    ")\n",
    "\n",
    "# Fit the model: Correctly passes feature_names and the time step 't=dt'\n",
    "model_custom.fit(Theta, x_dot=X_dot, feature_names=custom_feature_names, t=dt)\n",
    "\n",
    "print(\"\\n*** Discovered Equations (Custom Library) ***\")\n",
    "model_custom.print(lhs=deriv_cols)\n",
    "\n",
    "\n",
    "# 4. SUMMARY AND COMPARISON ANALYSIS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"C. Summary of Feature Space Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Print feature sizes (Syntax corrected)\n",
    "print(f\"Polynomial Library Size (Degree 3, 4 vars): {len(model_poly.get_feature_names())}\")\n",
    "print(f\"Custom Feature Library Size: {len(model_custom.get_feature_names())}\")\n",
    "\n",
    "# Check model recovery sparsity\n",
    "print(\"\\nSparsity (Number of active terms):\")\n",
    "print(f\"Polynomial Model Active Terms: {model_poly.complexity}\")\n",
    "print(f\"Custom Model Active Terms: {model_custom.complexity}\")"
   ],
   "id": "6c00bf8986adf48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "A. SINDy with Standard Polynomial Library (Degree 3)\n",
      "======================================================================\n",
      "\n",
      "*** Discovered Equations (Polynomial Library) ***\n",
      "dx = -10.000 x + 10.000 y\n",
      "dy = 0.019 1 + 28.007 x + -1.019 y + -0.002 z + 0.002 x^2 + -0.001 x y + -1.000 x z + 0.025 x u + 0.012 y u + 0.002 z u + 0.092 u^2 + -0.001 x^2 u + 0.001 x y u + -0.019 x u^2 + 0.001 y u^2 + -0.003 z u^2 + -0.035 u^3\n",
      "dz = 27.020 1 + -6.013 z + 0.321 x^2 + 0.125 x z + 0.234 y^2 + -0.271 y z + 0.112 z^2 + 0.835 z u + -0.110 x^2 u + 0.583 x y u + -0.379 x z u + 4.414 x u^2 + 0.312 y z u + -0.898 y u^2 + -1.268 z u^2 + 10.293 u^3\n",
      "du = 0.078 z\n",
      "\n",
      "======================================================================\n",
      "B. SINDy with Custom Rich Feature Matrix (Your Theta)\n",
      "======================================================================\n",
      "\n",
      "*** Discovered Equations (Custom Library) ***\n",
      "dx = 8.848 y + -38.752 u + 0.505 u^2 + -0.241 xu + 0.051 yu + -85.609 x*du + 81.262 y*du + 19.208 u*du + -8.072 dx*du\n",
      "dy = 4.050 x + 5.475 y + -0.194 xu + -1.910 x*du + -2.770 y*du + 0.014 u*dy + 0.496 dy*du\n",
      "dz = -2.667 z + 1.000 x^2 + 0.100 x*dx\n",
      "du = -0.416 z + 0.250 z*du\n",
      "\n",
      "======================================================================\n",
      "C. Summary of Feature Space Comparison\n",
      "======================================================================\n",
      "Polynomial Library Size (Degree 3, 4 vars): 35\n",
      "Custom Feature Library Size: 58\n",
      "\n",
      "Sparsity (Number of active terms):\n",
      "Polynomial Model Active Terms: 39\n",
      "Custom Model Active Terms: 21\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In cell 2, the equations are way too big to be interpreted.\n",
    "\n",
    "I tried changing the library polynomial degree from 3 to 2. the change is in cell 3 line 26)"
   ],
   "id": "7fa6b3947cce287e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:18:03.089321Z",
     "start_time": "2025-10-28T20:18:02.998148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Cell 3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pysindy as ps\n",
    "import pandas as pd\n",
    "\n",
    "# Load feature space by coping the path of the Excel done in cell 1\n",
    "feature_path = r\"C:\\Users\\braid\\OneDrive\\Desktop\\lorenze_attractor.csv\\rich_feature_matrix_small.csv\"\n",
    "feature_df = pd.read_csv(feature_path)\n",
    "\n",
    "# Select state variables that I'll predict\n",
    "state_vars = ['x', 'y', 'z', 'u']\n",
    "deriv_vars = ['dx', 'dy', 'dz', 'du']\n",
    "\n",
    "X = feature_df[state_vars].to_numpy()\n",
    "dXdt = feature_df[deriv_vars].to_numpy()\n",
    "t = feature_df['t'].to_numpy()\n",
    "dt = np.mean(np.diff(t))\n",
    "\n",
    "# Normalize the features\n",
    "X_mean, X_std = np.mean(X, axis=0), np.std(X, axis=0)\n",
    "X_norm = (X - X_mean) / X_std\n",
    "\n",
    "# Defining SINDY\n",
    "optimizer = ps.STLSQ(threshold=0.1, alpha=0.5)\n",
    "library = ps.PolynomialLibrary(degree=2)\n",
    "model = ps.SINDy(optimizer=optimizer, feature_library=library)\n",
    "\n",
    "model.fit(X_norm, t=dt, x_dot=dXdt, feature_names=state_vars)\n",
    "\n",
    "# Discovered Equations\n",
    "print(\"\\n SINDy Model Discovered:\\n\")\n",
    "for eq in model.equations():\n",
    "    print(eq)\n",
    "\n"
   ],
   "id": "25828348dc973fff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SINDy Model Discovered:\n",
      "\n",
      "-10.878 1 + -78.511 x + 99.609 y + 0.001 z + -0.007 u + -0.004 x^2 + 0.003 x y + 0.002 x z + -0.014 x u + -0.001 y z + 0.009 y u + -0.003 z u + -0.003 u^2\n",
      "7.538 1 + 50.323 x + -9.958 y + -10.816 z + 0.008 u + 0.001 x y + -71.183 x z + 0.013 x u + -0.001 y^2 + -0.009 y u + 0.004 z u + 0.004 u^2\n",
      "-57.471 1 + 0.845 x + 11.891 y + -24.187 z + 0.054 u + 0.026 x^2 + 78.181 x y + -0.007 x z + 0.100 x u + -0.001 y^2 + 0.007 y z + -0.066 y u + -0.004 z^2 + 0.025 z u + 0.025 u^2\n",
      "1.979 1\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The equations from cell 3 are sparse but still i dont think they're interpretable.\n",
    "\n",
    "So I'll try using different threshold."
   ],
   "id": "7171050714384d22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:18:03.183444Z",
     "start_time": "2025-10-28T20:18:03.105518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Cell 4\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pysindy as ps\n",
    "import pandas as pd\n",
    "\n",
    "# Load feature space by coping the path of the Excel done in cell 1\n",
    "feature_path = r\"C:\\Users\\braid\\OneDrive\\Desktop\\lorenze_attractor.csv\\rich_feature_matrix_small.csv\"\n",
    "feature_df = pd.read_csv(feature_path)\n",
    "\n",
    "# Select state variables that I'll predict\n",
    "state_vars = ['x', 'y', 'z', 'u']\n",
    "deriv_vars = ['dx', 'dy', 'dz', 'du']\n",
    "\n",
    "X = feature_df[state_vars].to_numpy()\n",
    "dXdt = feature_df[deriv_vars].to_numpy()\n",
    "t = feature_df['t'].to_numpy()\n",
    "dt = np.mean(np.diff(t))\n",
    "\n",
    "# Normalize the features\n",
    "X_mean, X_std = np.mean(X, axis=0), np.std(X, axis=0)\n",
    "X_norm = (X - X_mean) / X_std\n",
    "\n",
    "# Defining SINDY\n",
    "optimizer = ps.STLSQ(threshold=1, alpha=0.5)\n",
    "library = ps.PolynomialLibrary(degree=2)\n",
    "model = ps.SINDy(optimizer=optimizer, feature_library=library)\n",
    "\n",
    "model.fit(X_norm, t=dt, x_dot=dXdt, feature_names=state_vars)\n",
    "\n",
    "# Discovered Equations\n",
    "print(\"\\n SINDy Model Discovered:\\n\")\n",
    "for eq in model.equations():\n",
    "    print(eq)\n",
    "\n"
   ],
   "id": "200e0274cfde8f51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SINDy Model Discovered:\n",
      "\n",
      "-10.878 1 + -78.511 x + 99.609 y + 0.001 z + -0.007 u + -0.004 x^2 + 0.003 x y + 0.002 x z + -0.014 x u + -0.001 y z + 0.009 y u + -0.003 z u + -0.003 u^2\n",
      "7.540 1 + 50.318 x + -9.958 y + -10.814 z + 0.001 u + -0.004 x^2 + 0.003 x y + -71.182 x z + -0.001 y^2 + -0.001 y z + 0.001 z u + 0.001 u^2\n",
      "-57.471 1 + 0.845 x + 11.891 y + -24.187 z + 0.054 u + 0.026 x^2 + 78.181 x y + -0.007 x z + 0.100 x u + -0.001 y^2 + 0.007 y z + -0.066 y u + -0.004 z^2 + 0.025 z u + 0.025 u^2\n",
      "1.979 1\n"
     ]
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the output of cell 4, the equations discovered looks like they need regularization (too much variables and wide range of coefficients). And maybe try changing the optimizer of the SINDY and use L1 instead of sequentially threshold least square (STLSQ).\n",
    "In cell 5 I'll try using L1 on the output of sindy."
   ],
   "id": "d2fe3a47d11abe1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:18:03.218909Z",
     "start_time": "2025-10-28T20:18:03.190991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Cell 5: SINDy with Lasso (L1) Regularization (Applied to Degree 2 Feature Space)\n",
    "\n",
    "import numpy as np\n",
    "from pysindy import SINDy\n",
    "from pysindy.feature_library import PolynomialLibrary\n",
    "from sklearn.linear_model import Lasso # Corrected import\n",
    "\n",
    "# NOTE: This cell requires X, X_dot, dt, vars_basic, and deriv_cols from a previous cell.\n",
    "\n",
    "# Define the Optimizer\n",
    "# Using an alpha value (0.005) for moderate sparsity.\n",
    "LASSO_ALPHA = 0.1\n",
    "\n",
    "lasso_optimizer = Lasso(\n",
    "    alpha=LASSO_ALPHA,\n",
    "    max_iter=100000,\n",
    "    fit_intercept=False, # Set to False since PolynomialLibrary includes the bias term ('1')\n",
    "    copy_X=True,\n",
    "    precompute=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"SINDy with Lasso (L1) Regularization (Alpha={LASSO_ALPHA})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "#Define the Feature Library (Minimalist Degree 2)\n",
    "poly_library_deg2 = PolynomialLibrary(degree=2, include_bias=True)\n",
    "\n",
    "# Initialize and Fit the Model\n",
    "model_lasso = SINDy(\n",
    "    optimizer=lasso_optimizer,\n",
    "    feature_library=poly_library_deg2,\n",
    ")\n",
    "\n",
    "# Fit the model (requires X, X_dot, feature_names, and t=dt)\n",
    "model_lasso.fit(X, x_dot=X_dot, feature_names=vars_basic, t=dt)\n",
    "\n",
    "# Print Discovered Equations\n",
    "print(\"\\n*** Discovered Equations (Lasso L1 Optimizer, Alpha=0.005) ***\")\n",
    "model_lasso.print(lhs=deriv_cols)\n",
    "\n",
    "# Summary (Includes Complexity Calculation Fix)\n",
    "print(\"Summary: Lasso L1 Optimizer Results\")\n",
    "print(f\"Lasso Alpha (L1 Penalty Strength): {LASSO_ALPHA}\")\n",
    "\n",
    "xi_matrix = model_lasso.optimizer.coef_\n",
    "complexity_score = np.count_nonzero(xi_matrix)\n",
    "\n",
    "print(f\"Active Terms (Complexity): {complexity_score}\")"
   ],
   "id": "457f0c15af215583",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SINDy with Lasso (L1) Regularization (Alpha=0.1)\n",
      "======================================================================\n",
      "\n",
      "*** Discovered Equations (Lasso L1 Optimizer, Alpha=0.005) ***\n",
      "dx = -2.197 x + 5.419 y + 0.651 z + 0.203 x^2 + -0.009 x y + -0.181 x z + -0.048 y^2 + 0.068 y z + 0.434 y u + -0.034 z^2 + -0.145 z u + 1.275 u^2\n",
      "dy = 25.863 x + 0.135 y + -0.151 z + -0.048 x^2 + -0.951 x z + 0.015 y^2 + -0.021 y z + 0.009 z^2\n",
      "dz = -0.001 y + -2.645 z + 0.010 x^2 + 0.991 x y + -0.001 x z + 0.002 y^2 + -0.001 z^2\n",
      "du = 0.036 y + 0.216 z + 0.012 x^2 + -0.004 x y + 0.001 x z + -0.002 y^2 + -0.002 y z + -0.005 z^2\n",
      "Summary: Lasso L1 Optimizer Results\n",
      "Lasso Alpha (L1 Penalty Strength): 0.1\n",
      "Active Terms (Complexity): 36\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The equations in cell 5 resembles the lorenz system but, features with small coefficients should be dropped",
   "id": "131c65f509a0e941"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In cell 6 I'll try dropping features with small coefficients\n",
    "\n",
    "I'll compute the average absolute coefficient and drop the coefficients that falls below 5% of that threshold to zero"
   ],
   "id": "7da42d7425084b1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:19:05.132666Z",
     "start_time": "2025-10-28T20:19:05.127227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Cell 6: Calculate Average Absolute Coefficient Magnitude per Equation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Retrieve Coefficients from Lasso Model\n",
    "# Access the coefficients from the scikit-learn optimizer object\n",
    "xi_lasso = model_lasso.optimizer.coef_\n",
    "NUM_STATES = xi_lasso.shape[0] # Number of state variables (rows: dx, dy, dz, du)\n",
    "\n",
    "print(\"Average Absolute Coefficient Magnitude per Equation\")\n",
    "\n",
    "# Iterate and Calculate Average\n",
    "\n",
    "for i in range(NUM_STATES):\n",
    "    row_coeffs = xi_lasso[i, :]\n",
    "\n",
    "    # Isolate coefficients that are truly non-zero (above machine precision)\n",
    "    non_zero_coeffs = row_coeffs[np.abs(row_coeffs) > 1e-12]\n",
    "\n",
    "    if len(non_zero_coeffs) > 0:\n",
    "        # Calculate the average absolute magnitude\n",
    "        avg_abs_magnitude = np.mean(np.abs(non_zero_coeffs))\n",
    "\n",
    "        # Display the result using the pre-defined derivative names\n",
    "        print(f\"Equation {deriv_cols[i]}: Average Absolute Coefficient Magnitude = {avg_abs_magnitude:.6f}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Equation {deriv_cols[i]}: No non-zero coefficients found.\")\n"
   ],
   "id": "aa1d6c5e42e378a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Absolute Coefficient Magnitude per Equation\n",
      "Equation dx: Average Absolute Coefficient Magnitude = 0.888748\n",
      "Equation dy: Average Absolute Coefficient Magnitude = 3.399129\n",
      "Equation dz: Average Absolute Coefficient Magnitude = 0.456272\n",
      "Equation du: Average Absolute Coefficient Magnitude = 0.034794\n"
     ]
    }
   ],
   "execution_count": 156
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
